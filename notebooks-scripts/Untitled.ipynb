{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "import torchvision\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.transforms import ToTensor, Lambda, Compose, Normalize\n",
    "from collections import defaultdict\n",
    "from torch.utils.data import random_split\n",
    "import copy\n",
    "\n",
    "from NN_utils import *\n",
    "from NN_utils.train_and_eval import *\n",
    "from uncertainty import train_NN_with_g\n",
    "from uncertainty.losses import penalized_uncertainty\n",
    "import uncertainty.comparison as unc_comp\n",
    "import uncertainty.quantifications as unc\n",
    "\n",
    "\n",
    "from torch.utils.data import random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.cuda.is_available())\n",
    "dev = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms_train = transforms.Compose([\n",
    "                    transforms.RandomCrop(32, padding=4),\n",
    "                    transforms.RandomHorizontalFlip(),\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))])\n",
    "transforms_ = transforms.Compose([\n",
    "transforms.ToTensor(),\n",
    "transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = datasets.CIFAR100(\n",
    "root=\"data\",\n",
    " train=True,\n",
    " download=True,\n",
    "transform=transforms_train)\n",
    "\n",
    "test_data = datasets.CIFAR100(\n",
    "root=\"data\",\n",
    "train=False,\n",
    "download=True,\n",
    "transform=transforms_)\n",
    "\n",
    "#train_size = int(0.5*len(training_data))\n",
    "#val_size = len(training_data) - train_size\n",
    "#training_data, validation_data = random_split(training_data, [train_size, val_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_90 = dataset_cut_classes(training_data,indices = range(90))\n",
    "test_data_90 = dataset_cut_classes(test_data,indices = range(90))\n",
    "test_data_extra = dataset_cut_classes(test_data,indices = range(90,100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 12\n",
    "train_dataloader = DataLoader(training_data, batch_size=batch_size,shuffle = True)\n",
    "train_dataloader_90 = DataLoader(training_data_90, batch_size=batch_size,shuffle = True)\n",
    "#validation_dataloader = DataLoader(validation_data, batch_size=batch_size,shuffle = True)\n",
    "test_dataloader = DataLoader(test_data, batch_size=100)\n",
    "\n",
    "test_dataloader_90 = DataLoader(test_data_90, batch_size=100)\n",
    "test_dataloader_extra = DataLoader(test_data_extra, batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class hist_train():\n",
    "    def __init__(self,model,loss_criterion,data):\n",
    "        \n",
    "        self.model = model\n",
    "        self.loss_criterion = loss_criterion\n",
    "        self.data = data\n",
    "        \n",
    "        self.acc_list = []\n",
    "        self.loss_list = []\n",
    "        self.bce_iter = []\n",
    "    \n",
    "        \n",
    "    def update_hist(self,data = None):\n",
    "        \n",
    "        if data is None:\n",
    "            data = self.data\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for image,label in data:\n",
    "                image,label = image.to(dev),label.to(dev)\n",
    "                output = self.model(image)\n",
    "                loss = self.loss_criterion(output,label).item()\n",
    "                acc = correct_total(output,label)/label.size(0)\n",
    "\n",
    "                self.acc_list.append(acc)\n",
    "                self.loss_list.append(loss)\n",
    "                \n",
    "    def __str__(self):\n",
    "        pass\n",
    "            \n",
    "                \n",
    "class hist_train_g(hist_train):\n",
    "    def __init__(self,model,loss_criterion,data,c = 0):\n",
    "        super().__init__(model,loss_criterion,data)\n",
    "        \n",
    "        self.c = c\n",
    "        self.g_list = []\n",
    "        if c>0:\n",
    "            self.acc_c_g = []\n",
    "            self.acc_c_mcp = []\n",
    "            \n",
    "    def update_hist(self,data = None):\n",
    "        \n",
    "        if data is None:\n",
    "            data = self.data\n",
    "            \n",
    "        with torch.no_grad():\n",
    "\n",
    "            label, output, g = accumulate_results(self.model,data)\n",
    "            loss = self.loss_criterion(output,label).item()\n",
    "            output = torch.exp(output)\n",
    "            acc = correct_total(output,label)/label.size(0)\n",
    "\n",
    "            self.g_list.append(torch.mean(g).item())\n",
    "            self.acc_list.append(acc)\n",
    "            self.loss_list.append(loss)\n",
    "\n",
    "            mcp = unc.MCP_unc(output)\n",
    "\n",
    "            if self.c>0:\n",
    "                self.acc_c_g.append(unc_comp.acc_coverage(output,label,1-g,self.c))\n",
    "                self.acc_c_mcp.append(unc_comp.acc_coverage(output,label,mcp,self.c))\n",
    "            \n",
    "        \n",
    "    \n",
    "class Trainer():\n",
    "    def __init__(self,model,optimizer,loss_criterion,training_data,validation_data):\n",
    "        super().__init__()\n",
    "        \n",
    "        loss_crit = copy.deepcopy(loss_criterion.criterion)\n",
    "        loss_crit.reduction = 'mean'\n",
    "        \n",
    "        self.hist_train = hist_train(model,loss_crit,training_data)\n",
    "        self.hist_val = hist_train(model,loss_crit,validation_data)\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.loss_fn = loss_criterion\n",
    "\n",
    "    def fit(self,data,n_epochs):\n",
    "        loss_criterion = copy.deepcopy(self.loss_fn.criterion)\n",
    "        loss_criterion.reduction = 'mean'\n",
    "        for epoch in range(1,n_epochs+1):\n",
    "            train_NN(self.model,self.optimizer,data,loss_criterion,1, set_train_mode = True)\n",
    "            self.hist_train.update_hist()\n",
    "            self.hist_val.update_hist()\n",
    "            \n",
    "            \n",
    "            \n",
    "class Trainer_with_g(Trainer):\n",
    "    def __init__(self,model,optimizer,loss_fn,training_data,validation_data, c = 0.2):\n",
    "        super().__init__(model,optimizer,loss_fn,training_data,validation_data)\n",
    "        \n",
    "        loss_criterion = copy.deepcopy(loss_fn.criterion)\n",
    "        loss_criterion.reduction = 'mean'\n",
    "        \n",
    "        self.hist_train = hist_train_g(model,loss_criterion,training_data)\n",
    "        self.hist_val = hist_train_g(model,loss_criterion,validation_data,c=c)\n",
    "    \n",
    "    def fit_all(self,data,n_epochs):\n",
    "        unfreeze_params(self.model)\n",
    "        for epoch in range(1,n_epochs+1):\n",
    "            train_NN_with_g(self.model,self.optimizer,data,self.loss_fn,n_epochs=1, print_loss = True,set_train_mode = True)\n",
    "            self.hist_train.update_hist()\n",
    "            self.hist_val.update_hist()\n",
    "            self.loss_fn.update_L0(self.hist_train.loss_list[-1])\n",
    "\n",
    "    def fit_g(self,data,n_epochs,ignored_layers = ['main_layer','classifier_layer']):\n",
    "        \n",
    "        \n",
    "        loss = self.hist_val.loss_list[-1]\n",
    "        self.loss_fn.update_L0(loss)\n",
    "        ignore_layers(self.model,ignored_layers, reset = True)\n",
    "        for epoch in range(1,n_epochs+1):\n",
    "            train_NN_with_g(self.model,self.optimizer,data,self.loss_fn,n_epochs=1, print_loss = True,set_train_mode = False)\n",
    "            self.hist_train.update_hist()\n",
    "            self.hist_val.update_hist()\n",
    "            ignore_layers(self.model,ignored_layers, reset = False)\n",
    "            loss = self.hist_val.loss_list[-1]\n",
    "            self.loss_fn.update_L0(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from NN_models.CIFAR100 import Model_CNN_100_with_g\n",
    "model_100 = Model_CNN_100_with_g()\n",
    "model_100.to(dev)\n",
    "\n",
    "from NN_models import Model_CNN_with_g\n",
    "model_90 = Model_CNN_with_g(90)\n",
    "model_90.to(dev);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_100 = Model_CNN_100_with_g()\n",
    "model_100.to(dev)\n",
    "optimizer = torch.optim.SGD(model_100.parameters(), lr=1e-3)\n",
    "\n",
    "loss_criterion = nn.NLLLoss(reduction = 'none')\n",
    "loss_fn = penalized_uncertainty(loss_criterion,np.log(10))\n",
    "\n",
    "model_trainer_100 = Trainer_with_g(model_100,optimizer,loss_fn, train_dataloader,validation_dataloader,c = 0.2)\n",
    "model_trainer_100.fit_all(train_dataloader,80)\n",
    "acc, g, bce = model_metrics(model_100,loss_criterion,train_dataloader)\n",
    "print('Conjunto de treinamento: acc = ', acc, 'média de g = ', g, 'média de bce = ', bce, '\\n')\n",
    "acc, g, bce = model_metrics(model_100,loss_criterion,test_dataloader)\n",
    "print('Conjunto de teste: acc = ', acc, 'média de g = ', g, 'média de bce = ', bce, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_90 = Model_CNN_with_g(90)\n",
    "model_90.to(dev);\n",
    "optimizer = torch.optim.SGD(model_90.parameters(), lr=1e-3)\n",
    "\n",
    "loss_criterion = nn.NLLLoss(reduction = 'none')\n",
    "loss_fn = penalized_uncertainty(loss_criterion,np.log(10))\n",
    "\n",
    "model_trainer_90 = Trainer_with_g(model_90,optimizer,loss_fn, train_dataloader,validation_dataloader,c = 0.2)\n",
    "model_trainer_90.fit_all(train_dataloader,80)\n",
    "acc, g, bce = model_metrics(model_90,loss_criterion,train_dataloader)\n",
    "print('Conjunto de treinamento: acc = ', acc, 'média de g = ', g, 'média de bce = ', bce, '\\n')\n",
    "acc, g, bce = model_metrics(model_90,loss_criterion,test_dataloader)\n",
    "print('Conjunto de teste: acc = ', acc, 'média de g = ', g, 'média de bce = ', bce, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
