{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "import torchvision\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.transforms import ToTensor, Lambda, Compose, Normalize\n",
    "from collections import defaultdict\n",
    "from torch.utils.data import random_split\n",
    "import copy\n",
    "\n",
    "from NN_utils import *\n",
    "from NN_utils.train_and_eval import *\n",
    "from uncertainty import train_NN_with_g\n",
    "from uncertainty.losses import penalized_uncertainty\n",
    "import uncertainty.comparison as unc_comp\n",
    "import uncertainty.quantifications as unc\n",
    "\n",
    "\n",
    "from torch.utils.data import random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())\n",
    "dev = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms_train = transforms.Compose([\n",
    "                    transforms.RandomCrop(32, padding=4),\n",
    "                    transforms.RandomHorizontalFlip(),\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))])\n",
    "transforms_ = transforms.Compose([\n",
    "transforms.ToTensor(),\n",
    "transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "training_data = datasets.CIFAR10(\n",
    "root=\"data\",\n",
    " train=True,\n",
    " download=True,\n",
    "transform=transforms_train)\n",
    "\n",
    "test_data = datasets.CIFAR10(\n",
    "root=\"data\",\n",
    "train=False,\n",
    "download=True,\n",
    "transform=transforms_)\n",
    "\n",
    "train_size = int(0.5*len(training_data))\n",
    "val_size = len(training_data) - train_size\n",
    "training_data, validation_data = random_split(training_data, [train_size, val_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 12\n",
    "train_dataloader = DataLoader(training_data, batch_size=batch_size,shuffle = True)\n",
    "validation_dataloader = DataLoader(validation_data, batch_size=batch_size,shuffle = True)\n",
    "test_dataloader = DataLoader(test_data, batch_size=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from NN_models.CIFAR10 import Model_CNN_10_with_g as Model_CNN_with_g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer(torch.nn.Module):\n",
    "    def __init__(self,model,optimizer,loss_criterion, print_loss = True,keep_hist = True):\n",
    "        # adaptar keep hist para definir oq manter\\n\",\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.loss_fn = loss_criterion\n",
    "        self.print_loss = print_loss\n",
    "        self.keep_hist = keep_hist\n",
    "\n",
    "        self.g_list = []\n",
    "        self.acc_list = []\n",
    "        self.bce_list = []\n",
    "        self.bce_iter = []\n",
    "        self.acc_c_g = []\n",
    "        self.acc_c_mcp = []\n",
    "        \n",
    "    def fit(self,data,n_epochs):\n",
    "        for epoch in range(1,n_epochs+1):\n",
    "            self.train_NN_with_g(data,1,set_train_mode = True)\n",
    "            if self.keep_hist:\n",
    "                self.update_hist(data)\n",
    "            self.loss_fn.update_L0(self.bce_list[-1])\n",
    "            #self.bce_iter = []\n",
    "\n",
    "    def fit_x(self,data,n_epochs,ignored_layers = ['fc_g_layer']):\n",
    "        loss_criterion = copy.copy(self.loss_fn.criterion)\n",
    "        loss_criterion.reduction = 'mean'\n",
    "        ignore_layers(self.model,ignored_layers,reset = True)\n",
    "        for epoch in range(1,n_epochs+1):\n",
    "            train_NN(self.model,self.optimizer,data,loss_criterion,1, self.print_loss, set_train_mode = True)\n",
    "            if self.keep_hist:\n",
    "                self.update_hist(data)\n",
    "                ignore_layers(self.model,ignored_layers,reset = True)\n",
    "\n",
    "    def fit_g(self,data,n_epochs,ignored_layers = ['conv_layer','fc_layer','fc_x_layer']):\n",
    "        \n",
    "        _,_,bce = model_metrics(self.model,nn.NLLLoss(),data)\n",
    "        self.loss_fn.update_L0(bce)#(self.bce_list[-1])\n",
    "        ignore_layers(self.model,ignored_layers, reset = True)\n",
    "        for epoch in range(1,n_epochs+1):\n",
    "            self.train_NN_with_g(data,1, set_train_mode = False)\n",
    "            if self.keep_hist:\n",
    "                self.update_hist(data)\n",
    "                ignore_layers(self.model,ignored_layers, reset = True)\n",
    "        \n",
    "            \n",
    "    def train_NN_with_g(self,data,n_epochs, set_train_mode = True):\n",
    "            '''Train a NN that has a g layer'''\n",
    "            dev = next(self.model.parameters()).device\n",
    "            if set_train_mode:\n",
    "                self.model.train()\n",
    "            for epoch in range(n_epochs):\n",
    "                running_loss = 0\n",
    "                for image,label in data:\n",
    "                    image,label = image.to(dev), label.to(dev)\n",
    "                    self.optimizer.zero_grad()\n",
    "                    output = self.model(image)\n",
    "                    g = self.model.get_g()\n",
    "                    loss = self.loss_fn(output,g,label)\n",
    "                    self.bce_iter.append(torch.mean(self.loss_fn.criterion(output,label)).item())\n",
    "                    self.loss_fn.update_L0(np.mean(self.bce_iter[-2080:]))\n",
    "                    #_, _, bce =  model_metrics(self.model,self.loss_fn.criterion,data)\n",
    "                    #self.loss_fn.update_L0(bce)\n",
    "                    \n",
    "                    loss.backward()\n",
    "                    self.optimizer.step()\n",
    "                    running_loss += loss.item()\n",
    "                if self.print_loss:\n",
    "                    print('Epoch ', epoch+1, ', loss = ', running_loss/len(data))\n",
    "                    \n",
    "    def update_hist(self,data):\n",
    "        with torch.no_grad():\n",
    "            loss_criterion = copy.copy(self.loss_fn.criterion)\n",
    "            loss_criterion.reduction = 'mean'\n",
    "            #acc, g, bce =  model_metrics(self.model,loss_criterion,data)\n",
    "            #self.g_list.append(g)\n",
    "            #self.acc_list.append(acc)\n",
    "            #self.bce_list.append(bce)\n",
    "\n",
    "            label, output, g = accumulate_results(self.model,data)\n",
    "            bce = loss_criterion(output,label).item()\n",
    "            output = torch.exp(output)\n",
    "            mcp = unc.MCP_unc(output)\n",
    "\n",
    "            self.acc_c_g.append(unc_comp.acc_coverage(output,label,1-g,0.2))\n",
    "            self.acc_c_mcp.append(unc_comp.acc_coverage(output,label,mcp,0.2))\n",
    "\n",
    "            self.g_list.append(torch.mean(g).item())\n",
    "            acc = correct_total(output,label)/label.size(0)\n",
    "            self.acc_list.append(acc)\n",
    "            self.bce_list.append(bce)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_criterion = nn.NLLLoss(reduction = 'none')\n",
    "loss_fn = penalized_uncertainty(loss_criterion,np.log(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1 , loss =  2.2426244883642545\n",
      "Epoch  1 , loss =  1.9744338378910826\n",
      "Epoch  1 , loss =  1.771480081577905\n",
      "Epoch  1 , loss =  1.6492122118280854\n",
      "Epoch  1 , loss =  1.5608549471837316\n",
      "Epoch  1 , loss =  1.4859781486631125\n",
      "Epoch  1 , loss =  1.4065913258266998\n",
      "Epoch  1 , loss =  1.345102991601327\n",
      "Epoch  1 , loss =  1.2847553878850992\n",
      "Epoch  1 , loss =  1.218910352453847\n",
      "Epoch  1 , loss =  1.162887529682747\n",
      "Epoch  1 , loss =  1.1157685301754616\n",
      "Epoch  1 , loss =  1.0640397767988596\n",
      "Epoch  1 , loss =  1.0298798702900331\n",
      "Epoch  1 , loss =  0.9849911584534938\n",
      "Epoch  1 , loss =  0.9583527671312645\n",
      "Epoch  1 , loss =  0.9367534164661067\n",
      "Epoch  1 , loss =  0.9061683203002542\n",
      "Epoch  1 , loss =  0.8837059345060598\n",
      "Epoch  1 , loss =  0.8583660617599446\n",
      "Epoch  1 , loss =  0.8406986635385686\n",
      "Epoch  1 , loss =  0.8133808310839014\n",
      "Epoch  1 , loss =  0.8010108187051057\n",
      "Epoch  1 , loss =  0.7811012306463352\n",
      "Epoch  1 , loss =  0.7584868811263976\n",
      "Epoch  1 , loss =  0.7486157978092983\n",
      "Epoch  1 , loss =  0.7348201832271068\n",
      "Epoch  1 , loss =  0.7162798803921739\n",
      "Epoch  1 , loss =  0.7047578330489587\n",
      "Epoch  1 , loss =  0.6850606942810332\n",
      "Epoch  1 , loss =  0.6782871362486865\n",
      "Epoch  1 , loss =  0.6667432472352903\n",
      "Epoch  1 , loss =  0.6557309334425545\n",
      "Epoch  1 , loss =  0.6413209873525115\n",
      "Epoch  1 , loss =  0.6295607866587085\n",
      "Epoch  1 , loss =  0.617357670392076\n",
      "Epoch  1 , loss =  0.6116096146025779\n",
      "Epoch  1 , loss =  0.597864198712936\n",
      "Epoch  1 , loss =  0.5864358468842826\n",
      "Epoch  1 , loss =  0.5798259010313226\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-a2dc3c5ac71b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mmodel_trainer_3_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_3_2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mkeep_hist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mmodel_trainer_3_2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_x\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m40\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mmodel_trainer_3_2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_g\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidation_dataloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m40\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0macc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbce\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_3_2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloss_criterion\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-0eb0d64c5201>\u001b[0m in \u001b[0;36mfit_g\u001b[0;34m(self, data, n_epochs, ignored_layers)\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfit_g\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mignored_layers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'conv_layer'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'fc_layer'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'fc_x_layer'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbce\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNLLLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_fn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_L0\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#(self.bce_list[-1])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0mignore_layers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mignored_layers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "model_3_2 = Model_CNN_with_g()\n",
    "model_3_2 = model_3_2.to(dev)\n",
    "optimizer = torch.optim.SGD(model_3_2.parameters(), lr=1e-3)\n",
    "loss_fn = penalized_uncertainty(loss_criterion,np.log(10))\n",
    "\n",
    "model_trainer_3_2 = Trainer(model_3_2,optimizer,loss_fn, print_loss = True,keep_hist = True)\n",
    "model_trainer_3_2.fit_x(train_dataloader,40)\n",
    "model_trainer_3_2.fit_g(validation_dataloader,40)\n",
    "\n",
    "acc, g, bce = model_metrics(model_3_2,loss_criterion,train_dataloader)\n",
    "print('Conjunto de treinamento: acc = ', acc, 'média de g = ', g, 'média de bce = ', bce, '\\n')\n",
    "acc, g, bce = model_metrics(model_3_2,loss_criterion,test_dataloader)\n",
    "print('Conjunto de teste: acc = ', acc, 'média de g = ', g, 'média de bce = ', bce, '\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
