{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libs and pre-definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bibliotecas padrões python e utils pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "import torchvision\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.transforms import ToTensor, Lambda, Compose, Normalize\n",
    "from collections import defaultdict\n",
    "from torch.utils.data import random_split\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# Define o computador utilizado como cuda (gpu) se existir ou cpu caso contrário\n",
    "print(torch.cuda.is_available())\n",
    "dev = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": "true"
   },
   "source": [
    "### Bibliotecas desenvolvidas\n",
    "\n",
    "https://github.com/lfpc/Uncertainty_Estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'correct_total' from partially initialized module 'NN_utils.train_and_eval' (most likely due to a circular import) (/home/luis-felipe/anaconda3/lib/python3.8/site-packages/NN_utils/train_and_eval.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-7cd8fb9ca34a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mNN_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mNN_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_and_eval\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0muncertainty\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain_NN_with_g\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0muncertainty\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlosses\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpenalized_uncertainty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mNN_models\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mModel_CNN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/NN_utils/train_and_eval.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mNN_utils\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0muncertainty\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquantifications\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0munc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0muncertainty\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomparison\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0munc_comp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtrain_NN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloss_criterion\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mset_train_mode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/uncertainty/comparison.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mNN_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_and_eval\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcorrect_total\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mNN_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mapply_mask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'correct_total' from partially initialized module 'NN_utils.train_and_eval' (most likely due to a circular import) (/home/luis-felipe/anaconda3/lib/python3.8/site-packages/NN_utils/train_and_eval.py)"
     ]
    }
   ],
   "source": [
    "from NN_utils import *\n",
    "from NN_utils.train_and_eval import *\n",
    "from uncertainty import train_NN_with_g\n",
    "from uncertainty.losses import penalized_uncertainty\n",
    "from NN_models import Model_CNN\n",
    "import uncertainty.comparison as unc_comp\n",
    "import uncertainty.quantifications as unc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data download and transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms_train = transforms.Compose([\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.RandomCrop(32, padding=4),\n",
    "                    transforms.RandomHorizontalFlip(),\n",
    "                    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))])\n",
    "transforms_test = transforms.Compose([\n",
    "transforms.ToTensor(),\n",
    "transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "training_data = datasets.CIFAR10(\n",
    "root=\"data\",\n",
    " train=True,\n",
    " download=True,\n",
    "transform=transforms_train)\n",
    "\n",
    "test_data = datasets.CIFAR10(\n",
    "root=\"data\",\n",
    "train=False,\n",
    "download=True,\n",
    "transform=transforms_test)\n",
    "\n",
    "train_size = int(0.85*len(training_data))\n",
    "val_size = len(training_data) - train_size\n",
    "training_data, validation_data = random_split(training_data, [train_size, val_size])\n",
    "\n",
    "validation_data = copy.deepcopy(validation_data)\n",
    "validation_data.dataset.transform = transforms_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "train_dataloader = DataLoader(training_data, batch_size=batch_size,shuffle = True)\n",
    "validation_dataloader = DataLoader(validation_data, batch_size=batch_size,shuffle = False)\n",
    "test_dataloader = DataLoader(test_data, batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "425"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NN classes and Trainer class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": "true"
   },
   "source": [
    "### Definição da classe da rede neural"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model\n",
    "class Model_CNN(nn.Module):\n",
    "    \"\"\"CNN.\"\"\"\n",
    "\n",
    "    def __init__(self,n_classes=10):\n",
    "        \"\"\"CNN Builder.\"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        conv_layer = [\n",
    "\n",
    "            # Conv Layer block 1\n",
    "            nn.Conv2d(in_channels=3, out_channels=int(16), kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(int(16)),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(in_channels=int(16), out_channels=int(32), kernel_size=3, padding=1),\n",
    "            nn.Dropout(p=0.2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "        ]\n",
    "        \n",
    "        fc_layer = [\n",
    "            nn.Flatten(),\n",
    "            nn.Dropout(p=0.2),\n",
    "            nn.Linear(8192, int(1024)),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(int(1024), int(512)),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p=0.2)]\n",
    "        \n",
    "        main_layer = conv_layer+fc_layer\n",
    "        \n",
    "        self.main_layer = nn.Sequential(*main_layer)\n",
    "        \n",
    "        self.classifier_layer = nn.Sequential(\n",
    "            nn.Linear(int(512), n_classes),\n",
    "            nn.LogSoftmax(dim=1)\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Perform forward.\"\"\"\n",
    "    \n",
    "        x = self.main_layer(x)\n",
    "\n",
    "\n",
    "        y = self.classifier_layer(x)\n",
    "\n",
    "        y = y.float()\n",
    "        if not self.training:\n",
    "            y = torch.exp(y)\n",
    "        \n",
    "\n",
    "        \n",
    "        return y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model\n",
    "class Model_CNN_with_g(nn.Module):\n",
    "    \"\"\"CNN.\"\"\"\n",
    "\n",
    "    def __init__(self,n_classes=10):\n",
    "        \"\"\"CNN Builder.\"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        self.return_g = True\n",
    "        \n",
    "        conv_layer = [\n",
    "\n",
    "            # Conv Layer block 1\n",
    "            nn.Conv2d(in_channels=3, out_channels=int(16), kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(int(16)),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(in_channels=int(16), out_channels=int(32), kernel_size=3, padding=1),\n",
    "            nn.Dropout(p=0.2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "        ]\n",
    "        \n",
    "        fc_layer = [\n",
    "            nn.Flatten(),\n",
    "            nn.Dropout(p=0.2),\n",
    "            nn.Linear(8192, int(1024)),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(int(1024), int(512)),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p=0.2)]\n",
    "        \n",
    "        \n",
    "        main_layer = conv_layer+fc_layer\n",
    "        \n",
    "        self.main_layer = nn.Sequential(*main_layer)\n",
    "        \n",
    "\n",
    "        self.classifier_layer = nn.Sequential(\n",
    "            nn.Linear(int(512), n_classes),\n",
    "            nn.LogSoftmax(dim=1)\n",
    "        )\n",
    "        \n",
    "        '''self.fc_g_layer = nn.Sequential(\n",
    "            \n",
    "            nn.Linear(int(512), 1),\n",
    "            nn.Sigmoid()\n",
    "        )'''\n",
    "        \n",
    "        self.g_layer = nn.Sequential(\n",
    "            \n",
    "            nn.Linear(int(512), n_classes),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Perform forward.\"\"\"\n",
    "    \n",
    "        x = self.main_layer(x)\n",
    "\n",
    "\n",
    "        y = self.classifier_layer(x)\n",
    "\n",
    "        self.g = self.g_layer(x)\n",
    "        self.g = torch.max(self.g,dim=1).values\n",
    "\n",
    "        self.g = (self.g).float()\n",
    "        y = y.float()\n",
    "\n",
    "        \n",
    "        if self.return_g:\n",
    "            return y,self.g\n",
    "        else:\n",
    "            return y\n",
    "    \n",
    "    def get_g(self):\n",
    "        return self.g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model_CNN_with_g_2(nn.Module):\n",
    "    \"\"\"CNN.\"\"\"\n",
    "\n",
    "    def __init__(self,n_classes=10):\n",
    "        \"\"\"CNN Builder.\"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        self.return_g = True\n",
    "        \n",
    "        conv_layer = [\n",
    "\n",
    "            # Conv Layer block 1\n",
    "            nn.Conv2d(in_channels=3, out_channels=int(16), kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(int(16)),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(in_channels=int(16), out_channels=int(32), kernel_size=3, padding=1),\n",
    "            nn.Dropout(p=0.2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "        ]\n",
    "        \n",
    "        fc_layer = [\n",
    "            nn.Flatten(),\n",
    "            nn.Dropout(p=0.2),\n",
    "            nn.Linear(8192, int(1024)),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(int(1024), int(512)),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p=0.2)]\n",
    "        \n",
    "        \n",
    "        main_layer = conv_layer+fc_layer\n",
    "        \n",
    "        self.main_layer = nn.Sequential(*main_layer)\n",
    "        \n",
    "\n",
    "        self.classifier_layer = nn.Sequential(\n",
    "            nn.Linear(int(512), n_classes),\n",
    "            nn.LogSoftmax(dim=1)\n",
    "        )\n",
    "        \n",
    "        '''self.fc_g_layer = nn.Sequential(\n",
    "            \n",
    "            nn.Linear(int(512), 1),\n",
    "            nn.Sigmoid()\n",
    "        )'''\n",
    "        \n",
    "        self.g_layer = nn.Sequential(\n",
    "            \n",
    "            nn.Linear(n_classes, 64),\n",
    "            nn.ReLU(inplace=True), #tanh\n",
    "            nn.Dropout(p=0.2),\n",
    "            nn.Linear(64, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Perform forward.\"\"\"\n",
    "    \n",
    "        x = self.main_layer(x)\n",
    "\n",
    "\n",
    "        y = self.classifier_layer(x)\n",
    "\n",
    "        self.g = self.g_layer(y)\n",
    "\n",
    "        self.g = (self.g).float()\n",
    "        y = y.float()\n",
    "\n",
    "        \n",
    "        if self.return_g:\n",
    "            return y,self.g\n",
    "        else:\n",
    "            return y\n",
    "    \n",
    "    def get_g(self):\n",
    "        return self.g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model_CNN_with_g_selective(nn.Module):\n",
    "    \"\"\"CNN.\"\"\"\n",
    "\n",
    "    def __init__(self,n_classes=10):\n",
    "        \"\"\"CNN Builder.\"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        self.return_g = True\n",
    "        self.h = 0\n",
    "        \n",
    "        conv_layer = [\n",
    "\n",
    "            # Conv Layer block 1\n",
    "            nn.Conv2d(in_channels=3, out_channels=int(16), kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(int(16)),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(in_channels=int(16), out_channels=int(32), kernel_size=3, padding=1),\n",
    "            nn.Dropout(p=0.2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "        ]\n",
    "        \n",
    "        fc_layer = [\n",
    "            nn.Flatten(),\n",
    "            nn.Dropout(p=0.2),\n",
    "            nn.Linear(8192, int(1024)),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(int(1024), int(512)),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p=0.2)]\n",
    "        \n",
    "        \n",
    "        main_layer = conv_layer+fc_layer\n",
    "        \n",
    "        self.main_layer = nn.Sequential(*main_layer)\n",
    "        \n",
    "\n",
    "        self.classifier_layer = nn.Sequential(\n",
    "            nn.Linear(int(512), n_classes),\n",
    "            nn.LogSoftmax(dim=1)\n",
    "        )\n",
    "        \n",
    "        '''self.fc_g_layer = nn.Sequential(\n",
    "            \n",
    "            nn.Linear(int(512), 1),\n",
    "            nn.Sigmoid()\n",
    "        )'''\n",
    "        \n",
    "        self.g_layer = nn.Sequential(\n",
    "            \n",
    "            nn.Linear(512, 128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(128, 1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        self.auxiliary_layer = nn.Sequential(\n",
    "            nn.Linear(int(512), n_classes),\n",
    "            nn.LogSoftmax(dim=1)\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Perform forward.\"\"\"\n",
    "    \n",
    "        x = self.main_layer(x)\n",
    "\n",
    "\n",
    "        y = self.classifier_layer(x).float()\n",
    "\n",
    "        self.g = self.g_layer(x)\n",
    "\n",
    "        self.g = (self.g).float()\n",
    "        \n",
    "            \n",
    "        self.h = self.auxiliary_layer(x).float()\n",
    "        \n",
    "\n",
    "        \n",
    "        if self.return_g:\n",
    "            return y,self.g\n",
    "        else:\n",
    "            return y\n",
    "    \n",
    "    def get_g(self):\n",
    "        return self.g\n",
    "    def get_h(self):\n",
    "        return self.h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model_CNN_with_g_3(nn.Module):\n",
    "    \"\"\"CNN.\"\"\"\n",
    "\n",
    "    def __init__(self,n_classes=10):\n",
    "        \"\"\"CNN Builder.\"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        self.return_g = True\n",
    "        \n",
    "        conv_layer = [\n",
    "\n",
    "            # Conv Layer block 1\n",
    "            nn.Conv2d(in_channels=3, out_channels=int(16), kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(int(16)),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(in_channels=int(16), out_channels=int(32), kernel_size=3, padding=1),\n",
    "            nn.Dropout(p=0.2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "        ]\n",
    "        \n",
    "        fc_layer = [\n",
    "            nn.Flatten(),\n",
    "            nn.Dropout(p=0.2),\n",
    "            nn.Linear(8192, int(1024)),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(int(1024), int(512)),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p=0.2)]\n",
    "        \n",
    "        \n",
    "        main_layer = conv_layer+fc_layer\n",
    "        \n",
    "        self.main_layer = nn.Sequential(*main_layer)\n",
    "        \n",
    "\n",
    "        self.classifier_layer = nn.Sequential(\n",
    "            nn.Linear(int(512), n_classes),\n",
    "            nn.LogSoftmax(dim=1)\n",
    "        )\n",
    "        \n",
    "        '''self.fc_g_layer = nn.Sequential(\n",
    "            \n",
    "            nn.Linear(int(512), 1),\n",
    "            nn.Sigmoid()\n",
    "        )'''\n",
    "        \n",
    "        self.g_layer = nn.Sequential(\n",
    "            \n",
    "            nn.Linear(512, 128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(128, 32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(32, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Perform forward.\"\"\"\n",
    "    \n",
    "        x = self.main_layer(x)\n",
    "\n",
    "\n",
    "        y = self.classifier_layer(x)\n",
    "\n",
    "        self.g = self.g_layer(x)\n",
    "\n",
    "        self.g = (self.g).float()\n",
    "    \n",
    "            \n",
    "        \n",
    "\n",
    "        \n",
    "        if self.return_g:\n",
    "            return y,self.g\n",
    "        else:\n",
    "            return y\n",
    "    \n",
    "    def get_g(self):\n",
    "        return self.g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model_CNN_with_g_4(nn.Module): #realizar concatenação de x com y e etc\n",
    "    \"\"\"CNN.\"\"\"\n",
    "\n",
    "    def __init__(self,n_classes=10):\n",
    "        \"\"\"CNN Builder.\"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        self.return_g = True\n",
    "        \n",
    "        conv_layer = [\n",
    "\n",
    "            # Conv Layer block 1\n",
    "            nn.Conv2d(in_channels=3, out_channels=int(16), kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(int(16)),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(in_channels=int(16), out_channels=int(32), kernel_size=3, padding=1),\n",
    "            nn.Dropout(p=0.2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "        ]\n",
    "        \n",
    "        fc_layer = [\n",
    "            nn.Flatten(),\n",
    "            nn.Dropout(p=0.2),\n",
    "            nn.Linear(8192, int(1024)),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(int(1024), int(512)),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p=0.2)]\n",
    "        \n",
    "        \n",
    "        main_layer = conv_layer+fc_layer\n",
    "        \n",
    "        self.main_layer = nn.Sequential(*main_layer)\n",
    "        \n",
    "\n",
    "        self.classifier_layer = nn.Sequential(\n",
    "            nn.Linear(int(512), n_classes),\n",
    "            nn.LogSoftmax(dim=1)\n",
    "        )\n",
    "        \n",
    "        '''self.fc_g_layer = nn.Sequential(\n",
    "            \n",
    "            nn.Linear(int(512), 1),\n",
    "            nn.Sigmoid()\n",
    "        )'''\n",
    "        \n",
    "        self.g_layer_1 = nn.Sequential(\n",
    "            \n",
    "            nn.Linear(512, 64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(64, 10)\n",
    "        )\n",
    "        self.g_layer_2 = nn.Sequential(\n",
    "            \n",
    "            nn.Linear(20, 32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(32, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Perform forward.\"\"\"\n",
    "    \n",
    "        x = self.main_layer(x)\n",
    "\n",
    "\n",
    "        y = self.classifier_layer(x)\n",
    "\n",
    "        self.g = self.g_layer_1(x)\n",
    "        self.g = torch.cat((self.g,y),dim=1)\n",
    "        self.g = self.g_layer_2(self.g)\n",
    "\n",
    "        self.g = (self.g).float()\n",
    "        y = y.float()\n",
    "        \n",
    "        if self.return_g:\n",
    "            return y,self.g\n",
    "        else:\n",
    "            return y\n",
    "    \n",
    "    def get_g(self):\n",
    "        return self.g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": "true"
   },
   "source": [
    "### Definição das classes de treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class hist_train():\n",
    "\n",
    "    '''Accumulate results while training. Every time update_hist() is called, \n",
    "    it evaluates the usefull metrics over the dataset data and stores it in a list.'''\n",
    "    def __init__(self,model,loss_criterion,data, c = 1.0):\n",
    "        \n",
    "        self.model = model\n",
    "        self.loss_criterion = loss_criterion\n",
    "        self.data = data\n",
    "        self.c = c #coverage\n",
    "        \n",
    "        self.acc_list = []\n",
    "        self.loss_list = []\n",
    "        if c<1:\n",
    "            #acc_c represents accuracy when the c most uncertain samples are ignored\n",
    "            self.acc_c_mcp = [] \n",
    "            self.acc_c_entropy = []\n",
    "\n",
    "    \n",
    "    def update_hist(self):\n",
    "        '''Update acc_list's and loss_list.\n",
    "        If coverage is defined (different than 1), updates acc_c lists'''\n",
    "        \n",
    "        dev = next(self.model.parameters()).device\n",
    "        self.model.eval()\n",
    "            \n",
    "        with torch.no_grad():\n",
    "            #y_pred and label are accumulated for all dataset so that accuracy by coverage can by calculated\n",
    "            y_pred,label = accumulate_results(self.model,self.data)\n",
    "            \n",
    "            loss = self.loss_criterion(y_pred,label).item()\n",
    "            acc = correct_total(y_pred,label)/label.size(0) #accuracy\n",
    "            self.acc_list.append(acc)\n",
    "            self.loss_list.append(loss)\n",
    "            \n",
    "            if self.c<1:\n",
    "                #acc_c represents accuracy when the c most uncertain samples are ignored\n",
    "                mcp = unc.MCP_unc(y_pred) #maximum softmax value\n",
    "                ent = entropy(y_pred) #entropy of softmax\n",
    "                self.acc_c_mcp.append(unc_comp.acc_coverage(y_pred,label,mcp,1-self.c))\n",
    "                self.acc_c_entropy.append(unc_comp.acc_coverage(y_pred,label,ent,1-self.c))\n",
    "\n",
    "            \n",
    "class Trainer():\n",
    "    '''Class for easily training/fitting a Pytorch's NN model. Creates 2 'hist' classes,\n",
    "    keeping usefull metrics and values.'''\n",
    "    def __init__(self,model,optimizer,loss_criterion,training_data,validation_data = None, c=1.0):\n",
    "\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.loss_fn = loss_criterion\n",
    "        self.epoch = 0\n",
    "        \n",
    "        self.hist_train = hist_train(model,loss_criterion,training_data, c=c)\n",
    "        if validation_data is not None:\n",
    "            self.hist_val = hist_train(model,loss_criterion,validation_data,c=c)\n",
    "            \n",
    "\n",
    "    def fit(self,data,n_epochs):\n",
    "        for e in range(1,n_epochs+1):\n",
    "            self.epoch += 1\n",
    "            loss = train_NN(self.model,self.optimizer,data,self.loss_fn,1, print_loss = False) #model.train applied internally here\n",
    "            print('Epoch ', self.epoch, ', loss = ', loss)\n",
    "\n",
    "            self.hist_train.update_hist()\n",
    "            try: self.hist_val.update_hist() #with try/except in case there is no validation hist class\n",
    "            except: pass\n",
    "            \n",
    "    def update_hist(self):\n",
    "        '''Updates hist classes.\n",
    "        Usefull to use before training to keep pre-training values.'''\n",
    "        self.hist_train.update_hist()\n",
    "        try: self.hist_val.update_hist() #with try/except in case there is no validation hist class\n",
    "        except: pass\n",
    "        \n",
    "            \n",
    "            \n",
    "class hist_train_g(hist_train):\n",
    "     '''Accumulate results while training. Every time update_hist() is called, \n",
    "    it evaluates the usefull metrics over the dataset data and stores it in a list.\n",
    "    Equal to hist_train class, but keeps g (uncertainty estimation) values'''\n",
    "    def __init__(self,model,loss_criterion,data,c = 1.0):\n",
    "        super().__init__(model,loss_criterion,data)\n",
    "        \n",
    "        self.c = c\n",
    "        self.g_list = []\n",
    "        if c>0:\n",
    "            self.acc_c_g = []\n",
    "            self.acc_c_mcp = []\n",
    "            self.acc_c_entropy = []\n",
    "            \n",
    "    def update_hist(self):\n",
    "        '''Update acc_list's and loss_list.\n",
    "        Redefined so it update also g_list and (possibly) acc_c_g'''\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            \n",
    "            #output and label are accumulated for all dataset so that accuracy by coverage can by calculated\n",
    "            output,label = accumulate_results_g(self.model,data)\n",
    "            y_pred,g = output\n",
    "            g = g.view(-1)\n",
    "            \n",
    "            loss = self.loss_criterion(output.to(dev),label.to(dev)).item()\n",
    "            acc = correct_total(y_pred,label)/label.size(0)\n",
    "            self.acc_list.append(acc)\n",
    "            self.loss_list.append(loss)\n",
    "\n",
    "            self.g_list.append(torch.mean(g).item())\n",
    "\n",
    "            if self.c<1:\n",
    "                #acc_c represents accuracy when the c most uncertain samples are ignored\n",
    "                mcp = unc.MCP_unc(y_pred) #maximum softmax value\n",
    "                ent = entropy(y_pred) #entropy of softmax\n",
    "                self.acc_c_g.append(unc_comp.acc_coverage(y_pred,label,1-g,1-self.c))\n",
    "                self.acc_c_mcp.append(unc_comp.acc_coverage(y_pred,label,mcp,1-self.c))\n",
    "                self.acc_c_entropy.append(unc_comp.acc_coverage(y_pred,label,ent,1-self.c))\n",
    "\n",
    "\n",
    "class Trainer_with_g(Trainer):\n",
    "    '''Class for easily training/fitting a Pytorch's NN model. Creates 2 'hist' classes,\n",
    "    keeping usefull metrics and values.\n",
    "    Identical to Trainer class but with method for training only g's layers.'''\n",
    "    def __init__(self,model,optimizer,loss_fn,training_data,validation_data = None, c = 0.8):\n",
    "        super().__init__(model,optimizer,loss_fn,training_data,validation_data)\n",
    "        \n",
    "        self.hist_train = hist_train_g(model,loss_fn,training_data, c=c)\n",
    "        if validation_data is not None:\n",
    "            self.hist_val = hist_train_g(model,loss_fn,validation_data,c=c)\n",
    "\n",
    "    def fit_g(self,data,n_epochs,ignored_layers = ['main_layer','classifier_layer']):\n",
    "        '''Train only the layer specific for g, freezing (disables grad and set eval mode) the rest'''\n",
    "        for e in range(1,n_epochs+1):\n",
    "            self.epoch += 1\n",
    "            self.model.train()\n",
    "            #ignore_layers is applied every iteration because 'update_hist method set model to eval mode'\n",
    "            ignore_layers(self.model,ignored_layers, reset = False) \n",
    "            train_NN(self.model,self.optimizer,data,self.loss_fn,n_epochs=1, print_loss = True,set_train_mode = False)\n",
    "            self.hist_train.update_hist()\n",
    "            try: self.hist_val.update_hist() #with try/except in case there is no validation hist class\n",
    "            except: pass\n",
    "        unfreeze_params(self.model) #unfreeze params to avoid future mistakes\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testes e treinamentos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": "true"
   },
   "source": [
    "### Definição da perda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_criterion = nn.NLLLoss(reduction = 'none')\n",
    "loss_fn = penalized_uncertainty(loss_criterion,np.log(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class aux_loss_fs(nn.Module):\n",
    "    \n",
    "    def __init__(self,loss_criterion):\n",
    "        super().__init__()\n",
    "        self.L0 = 0\n",
    "        self.criterion = loss_criterion\n",
    "        \n",
    "    def forward(self, output,y_true):\n",
    "        y_pred,g = output\n",
    "        g = g.view(-1)\n",
    "        y_pred = torch.exp(y_pred)\n",
    "        right = correct_class(y_pred,y_true).float()\n",
    "        #loss = torch.square(g.view(-1)-MCP)\n",
    "        loss = self.criterion(g,right)\n",
    "        loss = torch.mean(loss)\n",
    "        return loss\n",
    "    \n",
    "    def update_L0(self,new_L0):\n",
    "        with torch.no_grad():\n",
    "            self.L0 = new_L0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class aux_loss(nn.Module):\n",
    "    \n",
    "    def __init__(self,loss_criterion):\n",
    "        super().__init__()\n",
    "        self.L0 = 0\n",
    "        self.criterion = loss_criterion\n",
    "        \n",
    "    def forward(self, y_pred,g,y_true):\n",
    "        g = g.view(-1)\n",
    "        y_pred = torch.exp(y_pred)\n",
    "        MCP = unc.get_MCP(y_pred)\n",
    "        loss = torch.square(g.view(-1)-MCP)\n",
    "        loss = torch.mean(loss)\n",
    "        return loss\n",
    "    \n",
    "    def update_L0(self,new_L0):\n",
    "        with torch.no_grad():\n",
    "            self.L0 = new_L0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accumulate_results(model,data):\n",
    "    '''Accumulate output (of model) and label of a entire dataset.'''\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        dev = next(model.parameters()).device\n",
    "\n",
    "        output_list = torch.Tensor([])\n",
    "        label_list = torch.Tensor([])\n",
    "        g_list = torch.Tensor([])\n",
    "\n",
    "        for image,label in data:\n",
    "            image = image.to(dev)\n",
    "\n",
    "            output = model(image)\n",
    "            g_bool = isinstance(output, tuple)\n",
    "            \n",
    "            if g_bool:\n",
    "                output,g = output\n",
    "                g = g.view(-1).cpu()\n",
    "                g_list = torch.cat((g_list,g))\n",
    "\n",
    "            label_list = torch.cat((label_list,label.cpu()))\n",
    "            output_list = torch.cat((output_list,output.cpu()))\n",
    "            \n",
    "        if g_bool:    \n",
    "            output_list = (output_list,g_list)\n",
    "        \n",
    "    return output_list,label_list.long()\n",
    "\n",
    "def entropy(y_pred, reduction = 'none',eps = 1e-10):\n",
    "    '''Returns the entropy of a probabilities tensor.'''\n",
    "    \n",
    "    entropy = -y_pred*torch.log(y_pred+eps)\n",
    "    entropy = torch.sum(entropy,-1)\n",
    "    \n",
    "    \n",
    "    if reduction == 'mean':\n",
    "        entropy = torch.mean(entropy)\n",
    "    elif reduction == 'sum':\n",
    "        entropy = torch.sum(entropy)\n",
    "        \n",
    "    return entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Treinamento dos modelos\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classificador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model_CNN(10).cuda()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n",
    "loss_criterion = nn.NLLLoss(reduction = 'mean')\n",
    "\n",
    "model_trainer = Trainer(model,optimizer,loss_criterion, train_dataloader,validation_dataloader)\n",
    "model_trainer.fit(train_dataloader,2000)\n",
    "state_dict  = model.state_dict()\n",
    "\n",
    "acc = model_acc(model,train_dataloader)\n",
    "print('Conjunto de treinamento: acc = ', acc)\n",
    "acc = model_acc(model,test_dataloader)\n",
    "print('Conjunto de teste: acc = ', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = r'/home/luis-felipe/Uncertainty_Estimation/torch_models'\n",
    "torch.save(model.state_dict(), PATH + 'model_classifier')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": "true"
   },
   "source": [
    "#### Perda padrão"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1 = Model_CNN_with_g()\n",
    "model_1 = model_1.to(dev)\n",
    "optimizer = torch.optim.SGD(model_1.parameters(), lr=1e-3)\n",
    "\n",
    "loss_criterion = nn.NLLLoss(reduction = 'none')\n",
    "loss_fn = penalized_uncertainty(loss_criterion,np.log(10))\n",
    "\n",
    "model_trainer_1 = Trainer_with_g(model_1,optimizer,loss_fn, train_dataloader,validation_dataloader,c = 0.2)\n",
    "model_trainer_1.fit_all(train_dataloader,200)\n",
    "acc, g, bce = model_metrics(model_1,loss_criterion,train_dataloader)\n",
    "print('Conjunto de treinamento: acc = ', acc, 'média de g = ', g, 'média de bce = ', bce, '\\n')\n",
    "acc, g, bce = model_metrics(model_1,loss_criterion,test_dataloader)\n",
    "print('Conjunto de teste: acc = ', acc, 'média de g = ', g, 'média de bce = ', bce, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_sep = Model_CNN_with_g()\n",
    "model_sep = model_sep.to(dev)\n",
    "model_sep.load_state_dict(state_dict,strict = False)\n",
    "optimizer = torch.optim.SGD(model_sep.parameters(), lr=1e-3)\n",
    "loss_criterion = nn.NLLLoss(reduction = 'none')\n",
    "loss_fn = penalized_uncertainty(loss_criterion,np.log(10))\n",
    "\n",
    "model_trainer_sep = Trainer_with_g(model_sep,optimizer,loss_fn, train_dataloader,validation_dataloader,c = 0.2)\n",
    "model_trainer_sep.hist_train = model_trainer.hist_train\n",
    "model_trainer_sep.hist_val = model_trainer.hist_val\n",
    "model_trainer_sep.hist_val.c = 0.2\n",
    "\n",
    "#model_trainer_sep.fit(train_dataloader,40)\n",
    "#model_trainer_sep.optimizer = torch.optim.SGD(model_sep.parameters(), lr=1e-2) #testar variações de lr\n",
    "model_trainer_sep.fit_g(validation_dataloader,200)\n",
    "\n",
    "acc, g, bce = model_metrics(model_sep,loss_criterion,train_dataloader)\n",
    "print('Conjunto de treinamento: acc = ', acc, 'média de g = ', g, 'média de bce = ', bce, '\\n')\n",
    "acc, g, bce = model_metrics(model_sep,loss_criterion,test_dataloader)\n",
    "print('Conjunto de teste: acc = ', acc, 'média de g = ', g, 'média de bce = ', bce, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2 = Model_CNN_with_g_2()\n",
    "model_2 = model_2.to(dev)\n",
    "optimizer = torch.optim.SGD(model_2.parameters(), lr=1e-3)\n",
    "loss_criterion = nn.NLLLoss(reduction = 'none')\n",
    "loss_fn = penalized_uncertainty(loss_criterion,np.log(10))\n",
    "\n",
    "model_trainer_2 = Trainer_with_g(model_2,optimizer,loss_fn, train_dataloader,validation_dataloader,c = 0.2)\n",
    "model_trainer_2.fit_all(train_dataloader,80)\n",
    "\n",
    "acc, g, bce = model_metrics(model_2,loss_criterion,train_dataloader)\n",
    "print('Conjunto de treinamento: acc = ', acc, 'média de g = ', g, 'média de bce = ', bce, '\\n')\n",
    "acc, g, bce = model_metrics(model_2,loss_criterion,test_dataloader)\n",
    "print('Conjunto de teste: acc = ', acc, 'média de g = ', g, 'média de bce = ', bce, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#modelo 2 # testar com tanh\n",
    "model_sep_2 = Model_CNN_with_g_2()\n",
    "model_sep_2 = model_sep_2.to(dev)\n",
    "model_sep_2.load_state_dict(state_dict,strict = False)\n",
    "optimizer = torch.optim.SGD(model_sep_2.parameters(), lr=1e-3)\n",
    "loss_criterion = nn.NLLLoss(reduction = 'none')\n",
    "loss_fn = penalized_uncertainty(loss_criterion,np.log(10))\n",
    "\n",
    "model_trainer_sep_2 = Trainer_with_g(model_sep_2,optimizer,loss_fn, train_dataloader,validation_dataloader,c = 0.2)\n",
    "model_trainer_sep_2.hist_train.acc_list = copy.copy(model_trainer.hist_train.acc_list)\n",
    "model_trainer_sep_2.hist_train.loss_list = copy.copy(model_trainer.hist_train.loss_list)\n",
    "model_trainer_sep_2.hist_val.acc_list = copy.copy(model_trainer.hist_val.acc_list)\n",
    "model_trainer_sep_2.hist_val.loss_list = copy.copy(model_trainer.hist_val.loss_list)\n",
    "#model_trainer_sep_2.fit(train_dataloader,40)\n",
    "model_trainer_sep_2.fit_g(train_dataloader,200)\n",
    "\n",
    "acc, g, bce = model_metrics(model_sep_2,loss_criterion,train_dataloader)\n",
    "print('Conjunto de treinamento: acc = ', acc, 'média de g = ', g, 'média de bce = ', bce, '\\n')\n",
    "acc, g, bce = model_metrics(model_sep_2,loss_criterion,test_dataloader)\n",
    "print('Conjunto de teste: acc = ', acc, 'média de g = ', g, 'média de bce = ', bce, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_3 = Model_CNN_with_g_3()\n",
    "model_3 = model_3.to(dev)\n",
    "optimizer = torch.optim.SGD(model_3.parameters(), lr=1e-3)\n",
    "loss_criterion = nn.NLLLoss(reduction = 'none')\n",
    "loss_fn = penalized_uncertainty(loss_criterion,np.log(10))\n",
    "\n",
    "model_trainer_3 = Trainer_with_g(model_3,optimizer,loss_fn, train_dataloader,validation_dataloader,c = 0.2)\n",
    "model_trainer_3.fit_all(train_dataloader,80)\n",
    "\n",
    "acc, g, bce = model_metrics(model_3,loss_criterion,train_dataloader)\n",
    "print('Conjunto de treinamento: acc = ', acc, 'média de g = ', g, 'média de bce = ', bce, '\\n')\n",
    "acc, g, bce = model_metrics(model_3,loss_criterion,test_dataloader)\n",
    "print('Conjunto de teste: acc = ', acc, 'média de g = ', g, 'média de bce = ', bce, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#modelo 3\n",
    "model_sep_3 = Model_CNN_with_g_3()\n",
    "model_sep_3 = model_sep_3.to(dev)\n",
    "model_sep_3.load_state_dict(state_dict,strict = False)\n",
    "optimizer = torch.optim.SGD(model_sep_3.parameters(), lr=1e-3)\n",
    "loss_criterion = nn.NLLLoss(reduction = 'none')\n",
    "loss_fn = penalized_uncertainty(loss_criterion,np.log(10))\n",
    "\n",
    "model_trainer_sep_3 = Trainer_with_g(model_sep_3,optimizer,loss_fn, train_dataloader,validation_dataloader,c = 0.2)\n",
    "model_trainer_sep_3.hist_train.acc_list = copy.copy(model_trainer.hist_train.acc_list)\n",
    "model_trainer_sep_3.hist_train.loss_list = copy.copy(model_trainer.hist_train.loss_list)\n",
    "model_trainer_sep_3.hist_val.acc_list = copy.copy(model_trainer.hist_val.acc_list)\n",
    "model_trainer_sep_3.hist_val.loss_list = copy.copy(model_trainer.hist_val.loss_list)\n",
    "#model_trainer_sep_2.fit(train_dataloader,40)\n",
    "model_trainer_sep_3.fit_g(train_dataloader,200)\n",
    "\n",
    "acc, g, bce = model_metrics(model_sep_3,loss_criterion,train_dataloader)\n",
    "print('Conjunto de treinamento: acc = ', acc, 'média de g = ', g, 'média de bce = ', bce, '\\n')\n",
    "acc, g, bce = model_metrics(model_sep_3,loss_criterion,test_dataloader)\n",
    "print('Conjunto de teste: acc = ', acc, 'média de g = ', g, 'média de bce = ', bce, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_4 = Model_CNN_with_g_4()\n",
    "model_4 = model_4.to(dev)\n",
    "optimizer = torch.optim.SGD(model_4.parameters(), lr=1e-3)\n",
    "loss_criterion = nn.NLLLoss(reduction = 'none')\n",
    "loss_fn = penalized_uncertainty(loss_criterion,np.log(10))\n",
    "\n",
    "model_trainer_4 = Trainer_with_g(model_4,optimizer,loss_fn, train_dataloader,validation_dataloader,c = 0.2)\n",
    "model_trainer_4.fit_all(train_dataloader,80)\n",
    "\n",
    "acc, g, bce = model_metrics(model_4,loss_criterion,train_dataloader)\n",
    "print('Conjunto de treinamento: acc = ', acc, 'média de g = ', g, 'média de bce = ', bce, '\\n')\n",
    "acc, g, bce = model_metrics(model_4,loss_criterion,test_dataloader)\n",
    "print('Conjunto de teste: acc = ', acc, 'média de g = ', g, 'média de bce = ', bce, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#modelo 2\n",
    "model_sep_4 = Model_CNN_with_g_4()\n",
    "model_sep_4 = model_sep_4.to(dev)\n",
    "model_sep_4.load_state_dict(state_dict,strict = False)\n",
    "optimizer = torch.optim.SGD(model_sep_4.parameters(), lr=1e-3)\n",
    "loss_criterion = nn.NLLLoss(reduction = 'none')\n",
    "loss_fn = penalized_uncertainty(loss_criterion,np.log(10))\n",
    "\n",
    "model_trainer_sep_4 = Trainer_with_g(model_sep_4,optimizer,loss_fn, train_dataloader,validation_dataloader,c = 0.2)\n",
    "model_trainer_sep_4.hist_train.acc_list = model_trainer.hist_train.acc_list\n",
    "model_trainer_sep_4.hist_train.loss_list = model_trainer.hist_train.loss_list\n",
    "model_trainer_sep_4.hist_val.acc_list = model_trainer.hist_val.acc_list\n",
    "model_trainer_sep_4.hist_val.loss_list = model_trainer.hist_val.loss_list\n",
    "#model_trainer_sep_2.fit(train_dataloader,40)\n",
    "model_trainer_sep_4.fit_g(train_dataloader,200)\n",
    "\n",
    "acc, g, bce = model_metrics(model_sep_4,loss_criterion,train_dataloader)\n",
    "print('Conjunto de treinamento: acc = ', acc, 'média de g = ', g, 'média de bce = ', bce, '\\n')\n",
    "acc, g, bce = model_metrics(model_sep_4,loss_criterion,test_dataloader)\n",
    "print('Conjunto de teste: acc = ', acc, 'média de g = ', g, 'média de bce = ', bce, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": "true"
   },
   "source": [
    "#### Perda adaptada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#modelo 2\n",
    "model_sep_mcp = Model_CNN_with_g_2()\n",
    "model_sep_mcp = model_sep_mcp.to(dev)\n",
    "optimizer = torch.optim.SGD(model_sep_mcp.parameters(), lr=1e-3)\n",
    "loss_fn = aux_loss(loss_criterion)#penalized_uncertainty(loss_criterion,np.log(10))\n",
    "\n",
    "model_trainer_sep_mcp = Trainer_with_g(model_sep_mcp,optimizer,loss_fn, train_dataloader,validation_dataloader,c = 0.2)\n",
    "model_trainer_sep_mcp.fit(train_dataloader,40)\n",
    "model_trainer_sep_mcp.fit_g(validation_dataloader,200)\n",
    "\n",
    "acc, g, bce = model_metrics(model_sep_mcp,loss_criterion,train_dataloader)\n",
    "print('Conjunto de treinamento: acc = ', acc, 'média de g = ', g, 'média de bce = ', bce, '\\n')\n",
    "acc, g, bce = model_metrics(model_sep_mcp,loss_criterion,test_dataloader)\n",
    "print('Conjunto de teste: acc = ', acc, 'média de g = ', g, 'média de bce = ', bce, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#modelo fs\n",
    "model_sep_fs = Model_CNN_with_g_2()\n",
    "model_sep_fs = model_sep_fs.to(dev)\n",
    "model_sep_fs.load_state_dict(state_dict,strict = False)\n",
    "optimizer = torch.optim.SGD(model_sep_fs.parameters(), lr=1e-3)\n",
    "loss_criterion = nn.NLLLoss(reduction = 'none')\n",
    "loss_fn = aux_loss_fs(loss_criterion)#penalized_uncertainty(loss_criterion,np.log(10))\n",
    "\n",
    "model_trainer_sep_fs = Trainer_with_g(model_sep_fs,optimizer,loss_fn, train_dataloader,validation_dataloader,c = 0.2)\n",
    "#model_trainer_sep_fs.fit(train_dataloader,40)\n",
    "model_trainer_sep_fs.hist_train.acc_list = copy.copy(model_trainer.hist_train.acc_list)\n",
    "model_trainer_sep_fs.hist_train.loss_list = copy.copy(model_trainer.hist_train.loss_list)\n",
    "model_trainer_sep_fs.hist_val.acc_list = copy.copy(model_trainer.hist_val.acc_list)\n",
    "model_trainer_sep_fs.hist_val.loss_list = copy.copy(model_trainer.hist_val.loss_list)\n",
    "\n",
    "model_trainer_sep_fs.loss_fn.criterion = nn.BCELoss()\n",
    "model_trainer_sep_fs.fit_g(train_dataloader,900)\n",
    "\n",
    "acc, g, bce = model_metrics(model_sep_fs,loss_criterion,train_dataloader)\n",
    "print('Conjunto de treinamento: acc = ', acc, 'média de g = ', g, 'média de bce = ', bce, '\\n')\n",
    "acc, g, bce = model_metrics(model_sep_fs,loss_criterion,test_dataloader)\n",
    "print('Conjunto de teste: acc = ', acc, 'média de g = ', g, 'média de bce = ', bce, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#modelo fs\n",
    "model_sep_fs_3 = Model_CNN_with_g_3()\n",
    "model_sep_fs_3 = model_sep_fs_3.to(dev)\n",
    "model_sep_fs_3.load_state_dict(state_dict,strict = False)\n",
    "optimizer = torch.optim.SGD(model_sep_fs_3.parameters(), lr=1e-3)\n",
    "loss_criterion = nn.NLLLoss(reduction = 'none')\n",
    "loss_fn = aux_loss_fs(loss_criterion)#penalized_uncertainty(loss_criterion,np.log(10))\n",
    "\n",
    "model_trainer_sep_fs_3 = Trainer_with_g(model_sep_fs_3,optimizer,loss_fn, train_dataloader,validation_dataloader,c = 0.2)\n",
    "#model_trainer_sep_fs.fit(train_dataloader,40)\n",
    "model_trainer_sep_fs_3.hist_train.acc_list = copy.copy(model_trainer.hist_train.acc_list)\n",
    "model_trainer_sep_fs_3.hist_train.loss_list = copy.copy(model_trainer.hist_train.loss_list)\n",
    "model_trainer_sep_fs_3.hist_val.acc_list = copy.copy(model_trainer.hist_val.acc_list)\n",
    "model_trainer_sep_fs_3.hist_val.loss_list = copy.copy(model_trainer.hist_val.loss_list)\n",
    "\n",
    "model_trainer_sep_fs_3.loss_fn.criterion = nn.BCELoss()\n",
    "model_trainer_sep_fs_3.fit_g(train_dataloader,800)\n",
    "\n",
    "acc, g, bce = model_metrics(model_sep_fs_3,loss_criterion,train_dataloader)\n",
    "print('Conjunto de treinamento: acc = ', acc, 'média de g = ', g, 'média de bce = ', bce, '\\n')\n",
    "acc, g, bce = model_metrics(model_sep_fs_3,loss_criterion,test_dataloader)\n",
    "print('Conjunto de teste: acc = ', acc, 'média de g = ', g, 'média de bce = ', bce, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#modelo fs\n",
    "model_sep_fs_4 = Model_CNN_with_g_4()\n",
    "model_sep_fs_4 = model_sep_fs_4.to(dev)\n",
    "model_sep_fs_4.load_state_dict(state_dict,strict = False)\n",
    "optimizer = torch.optim.SGD(model_sep_fs_4.parameters(), lr=1e-3)\n",
    "loss_criterion = nn.NLLLoss(reduction = 'none')\n",
    "loss_fn = aux_loss_fs(loss_criterion)#penalized_uncertainty(loss_criterion,np.log(10))\n",
    "\n",
    "model_trainer_sep_fs_4 = Trainer_with_g(model_sep_fs_4,optimizer,loss_fn, train_dataloader,validation_dataloader,c = 0.2)\n",
    "#model_trainer_sep_fs.fit(train_dataloader,40)\n",
    "model_trainer_sep_fs_4.hist_train.acc_list = model_trainer.hist_train.acc_list\n",
    "model_trainer_sep_fs_4.hist_train.loss_list = model_trainer.hist_train.loss_list\n",
    "model_trainer_sep_fs_4.hist_val.acc_list = model_trainer.hist_val.acc_list\n",
    "model_trainer_sep_fs_4.hist_val.loss_list = model_trainer.hist_val.loss_list\n",
    "\n",
    "model_trainer_sep_fs_4.loss_fn.criterion = nn.BCELoss()\n",
    "model_trainer_sep_fs_4.fit_g(train_dataloader,800)\n",
    "\n",
    "acc, g, bce = model_metrics(model_sep_fs,loss_criterion,train_dataloader)\n",
    "print('Conjunto de treinamento: acc = ', acc, 'média de g = ', g, 'média de bce = ', bce, '\\n')\n",
    "acc, g, bce = model_metrics(model_sep_fs,loss_criterion,test_dataloader)\n",
    "print('Conjunto de teste: acc = ', acc, 'média de g = ', g, 'média de bce = ', bce, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Perda selective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy_const(w):\n",
    "    H = torch.exp(entropy(w,reduction = 'sum'))/w.size(0)\n",
    "    return H\n",
    "normalize_tensor = (lambda x: torch.nn.functional.normalize(x, p=1,dim=-1))\n",
    "\n",
    "mean_const = (lambda x: torch.mean(x, dim=-1))\n",
    "\n",
    "def IPM_selectivenet(r,const,lamb = 32):\n",
    "    #optimize x such that const >0\n",
    "    gama = lamb*torch.square(torch.maximum(torch.tensor([0]).cuda(),const))\n",
    "    objective = r + gama\n",
    "    return objective\n",
    "    \n",
    "#implementar 3 abordagens para baseline do selective net:\n",
    "#com cabeça H, com F no lugar da cabeça H, e sem cabeça auxiliar (artigo)\n",
    "class selective_net_2(torch.nn.Module):\n",
    "    def __init__(self,criterion,w_fn = normalize_tensor,c_fn = entropy_const,optim_method = IPM_selectivenet, c = 0.8,\n",
    "                 alpha = 1.0, head = 'y',const_var = 'w'):\n",
    "        super().__init__()\n",
    "\n",
    "        self.criterion = criterion #criterion must have reduction set to 'none'\n",
    "        self.w_fn = w_fn #transform applied to g\n",
    "        self.c_fn = c_fn #transform applied to w that goes onto constraint\n",
    "        self.optim_method = optim_method #transform applied to risk (loss) and constraint and returns a equivalent unconstrained objective\n",
    "        self.c = c #coverage\n",
    "        self.alpha = alpha\n",
    "        self.head = head\n",
    "        self.const_var = const_var\n",
    "    \n",
    "    def get_loss(self,y_pred,w,y_true):\n",
    "        \n",
    "        loss = w*self.criterion(y_pred,y_true)\n",
    "        loss = torch.sum(loss)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def get_constraint(self,w): \n",
    "        H = self.c_fn(w) #must be >= c\n",
    "        constraint = self.c - H #must be <=0\n",
    "        return constraint\n",
    "\n",
    "    def forward(self,output,y_true):\n",
    "        \n",
    "        y_pred,g = output\n",
    "        g = g.view(-1)\n",
    "        w = self.w_fn(g)\n",
    "        \n",
    "        loss = self.get_loss(y_pred,w,y_true)\n",
    "        if self.const_var == 'w':\n",
    "            const = self.get_constraint(w)\n",
    "        elif self.const_var == 'g':\n",
    "            const = self.get_constraint(g)\n",
    "        if self.optim_method is not None:\n",
    "            loss = self.optim_method(loss, const)\n",
    "            \n",
    "        if self.alpha != 1.0:\n",
    "            w = self.w_fn(torch.ones([torch.numel(g)])).to(y_pred.device)\n",
    "            if self.head == 'y':\n",
    "                loss_h = self.get_loss(y_pred,w,y_true)\n",
    "            else: \n",
    "                h = self.head()\n",
    "                loss_h = self.get_loss(h,w,y_true) if (h.size(0) == y_true.size(0)) else 0\n",
    "            loss = self.alpha*loss + (1-self.alpha)*loss_h\n",
    "            \n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_selective_noconst = Model_CNN_with_g_3(10).cuda()\n",
    "optimizer = torch.optim.SGD(model_selective_noconst.parameters(), lr=1e-3)\n",
    "loss_criterion = nn.NLLLoss(reduction = 'none')\n",
    "loss_fn = selective_net_2(loss_criterion,optim_method = None)\n",
    "\n",
    "model_trainer_selective_noconst = Trainer_with_g(model_selective_noconst,optimizer,loss_fn, train_dataloader,validation_dataloader,c = 0.2)\n",
    "model_trainer_selective_noconst.fit(train_dataloader,500)\n",
    "\n",
    "model_selective_noconst.return_g = False\n",
    "acc = model_acc(model_selective_noconst,train_dataloader)\n",
    "print('Conjunto de treinamento: acc = ', acc)\n",
    "acc = model_acc(model_selective_noconst,test_dataloader)\n",
    "print('Conjunto de teste: acc = ', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_selective = Model_CNN_with_g_3(10).cuda() #batch_size = 12\n",
    "optimizer = torch.optim.SGD(model_selective.parameters(), lr=1e-3)\n",
    "loss_criterion = nn.NLLLoss(reduction = 'none')\n",
    "loss_fn = selective_net_2(loss_criterion)\n",
    "\n",
    "model_trainer_selective = Trainer_with_g(model_selective,optimizer,loss_fn, train_dataloader,validation_dataloader,c = 0.2)\n",
    "model_trainer_selective.fit(train_dataloader,2000)\n",
    "\n",
    "model_selective.return_g = False\n",
    "acc = model_acc(model_selective,train_dataloader)\n",
    "print('Conjunto de treinamento: acc = ', acc)\n",
    "acc = model_acc(model_selective,test_dataloader)\n",
    "print('Conjunto de teste: acc = ', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_selective_100 = Model_CNN_with_g_3(10).cuda() #batch_size = 100\n",
    "optimizer = torch.optim.SGD(model_selective_100.parameters(), lr=1e-3)\n",
    "loss_criterion = nn.NLLLoss(reduction = 'none')\n",
    "loss_fn = selective_net_2(loss_criterion)\n",
    "\n",
    "model_trainer_selective_100 = Trainer_with_g(model_selective_100,optimizer,loss_fn, train_dataloader,validation_dataloader,c = 0.2)\n",
    "model_trainer_selective_100.fit(train_dataloader,500)\n",
    "\n",
    "model_selective_100.return_g = False\n",
    "acc = model_acc(model_selective_100,train_dataloader)\n",
    "print('Conjunto de treinamento: acc = ', acc)\n",
    "acc = model_acc(model_selective_100,test_dataloader)\n",
    "print('Conjunto de teste: acc = ', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_selective_sep_noconst = Model_CNN_with_g_3(10).cuda()\n",
    "model_selective_sep_noconst.load_state_dict(state_dict,strict = False)\n",
    "optimizer = torch.optim.SGD(model_selective_sep_noconst.parameters(), lr=1e-3)\n",
    "loss_criterion = nn.NLLLoss(reduction = 'none')\n",
    "loss_fn = selective_net_2(loss_criterion,optim_method = None)\n",
    "\n",
    "model_trainer_selective_sep_noconst = Trainer_with_g(model_selective_sep_noconst,optimizer,loss_fn, train_dataloader,validation_dataloader,c = 0.2)\n",
    "model_trainer_selective_sep_noconst.fit_g(train_dataloader,500)\n",
    "\n",
    "model_selective_sep_noconst.return_g = False\n",
    "acc = model_acc(model_selective_sep_noconst,train_dataloader)\n",
    "print('Conjunto de treinamento: acc = ', acc)\n",
    "acc = model_acc(model_selective_sep_noconst,test_dataloader)\n",
    "print('Conjunto de teste: acc = ', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_selective_sep = Model_CNN_with_g_3(10).cuda()\n",
    "model_selective_sep.load_state_dict(state_dict,strict = False)\n",
    "optimizer = torch.optim.SGD(model_selective_sep.parameters(), lr=1e-3)\n",
    "loss_criterion = nn.NLLLoss(reduction = 'none')\n",
    "loss_fn = selective_net_2(loss_criterion)\n",
    "\n",
    "model_trainer_selective_sep = Trainer_with_g(model_selective_sep,optimizer,loss_fn, train_dataloader,validation_dataloader,c = 0.2)\n",
    "model_trainer_selective_sep.fit_g(train_dataloader,500)\n",
    "\n",
    "model_selective_sep.return_g = False\n",
    "acc = model_acc(model_selective_sep,train_dataloader)\n",
    "print('Conjunto de treinamento: acc = ', acc)\n",
    "acc = model_acc(model_selective_sep,test_dataloader)\n",
    "print('Conjunto de teste: acc = ', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_selective_def_sep = Model_CNN_with_g_3(10).cuda()\n",
    "model_selective_def_sep.load_state_dict(state_dict,strict = False)\n",
    "optimizer = torch.optim.SGD(model_selective_def_sep.parameters(), lr=1e-3)\n",
    "loss_criterion = nn.NLLLoss(reduction = 'none')\n",
    "loss_fn = selective_net_2(loss_criterion, c_fn = mean_const)\n",
    "\n",
    "model_trainer_selective_def_sep = Trainer_with_g(model_selective_def_sep,optimizer,loss_fn, train_dataloader,validation_dataloader,c = 0.2)\n",
    "model_trainer_selective_def_sep.fit_g(train_dataloader,500)\n",
    "\n",
    "model_selective_def_sep.return_g = False\n",
    "acc = model_acc(model_selective_def_sep,train_dataloader)\n",
    "print('Conjunto de treinamento: acc = ', acc)\n",
    "acc = model_acc(model_selective_def_sep,test_dataloader)\n",
    "print('Conjunto de teste: acc = ', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_selective_def = Model_CNN_with_g_selective(10).cuda()\n",
    "optimizer = torch.optim.SGD(model_selective_def.parameters(), lr=1e-3)\n",
    "loss_criterion = nn.NLLLoss(reduction = 'none')\n",
    "loss_fn = selective_net_2(loss_criterion, c_fn = mean_const,alpha = 0.5,head = model_selective_def.get_h,const_var = 'g')\n",
    "\n",
    "model_trainer_selective_def = Trainer_with_g(model_selective_def,optimizer,loss_fn, train_dataloader,validation_dataloader,c = 0.2)\n",
    "model_trainer_selective_def.fit(train_dataloader,500)\n",
    "\n",
    "model_selective_def.return_g = False\n",
    "acc = model_acc(model_selective_def,train_dataloader)\n",
    "print('Conjunto de treinamento: acc = ', acc)\n",
    "acc = model_acc(model_selective_def,test_dataloader)\n",
    "print('Conjunto de teste: acc = ', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_selective_def_head_f = Model_CNN_with_g_selective(10).cuda()\n",
    "optimizer = torch.optim.SGD(model_selective_def_head_f.parameters(), lr=1e-3)\n",
    "loss_criterion = nn.NLLLoss(reduction = 'none')\n",
    "loss_fn = selective_net_2(loss_criterion, c_fn = mean_const,alpha = 0.5,head = 'y',const_var = 'g')\n",
    "\n",
    "model_trainer_selective_def_head_f = Trainer_with_g(model_selective_def_head_f,optimizer,loss_fn, train_dataloader,validation_dataloader,c = 0.2)\n",
    "model_trainer_selective_def_head_f.fit(train_dataloader,1000)\n",
    "\n",
    "model_selective_def_head_f.return_g = False\n",
    "acc = model_acc(model_selective_def_head_f,train_dataloader)\n",
    "print('Conjunto de treinamento: acc = ', acc)\n",
    "acc = model_acc(model_selective_def_head_f,test_dataloader)\n",
    "print('Conjunto de teste: acc = ', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_selective_def_no_head = Model_CNN_with_g_selective(10).cuda()\n",
    "optimizer = torch.optim.SGD(model_selective_def_no_head.parameters(), lr=1e-3)\n",
    "loss_criterion = nn.NLLLoss(reduction = 'none')\n",
    "loss_fn = selective_net_2(loss_criterion, c_fn = mean_const,const_var = 'g')\n",
    "\n",
    "model_trainer_selective_def_no_head = Trainer_with_g(model_selective_def_no_head,optimizer,loss_fn, train_dataloader,validation_dataloader,c = 0.2)\n",
    "model_trainer_selective_def_no_head.fit(train_dataloader,1000)\n",
    "\n",
    "model_selective_no_head.return_g = False\n",
    "acc = model_acc(model_selective_def_no_head,train_dataloader)\n",
    "print('Conjunto de treinamento: acc = ', acc)\n",
    "acc = model_acc(model_selective_def_no_head,test_dataloader)\n",
    "print('Conjunto de teste: acc = ', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": "true"
   },
   "source": [
    "## Plots e análises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {model:model_trainer,\n",
    "         model_selective: model_trainer_selective,\n",
    "          model_selective_noconst: model_trainer_selective_noconst,\n",
    "         model_selective_sep: model_trainer_selective_sep,\n",
    "         model_selective_sep_noconst: model_trainer_selective_sep_noconst,\n",
    "         model_selective_def:model_trainer_selective_def,\n",
    "         model_selective_def_no_head:model_trainer_selective_def_no_head}#,\n",
    "          #model_sep_2:model_trainer_sep_2,\n",
    "          #model_sep_4:model_trainer_sep_4,\n",
    "          #model_sep_fs:model_trainer_sep_fs,\n",
    "          #model_sep_fs_3:model_trainer_sep_fs_3,\n",
    "          #model_sep_fs_4:model_trainer_sep_fs_4,\n",
    "          #model_sep_3:model_trainer_sep_3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.hist_val.acc_c_mcp[1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod = model_selective_def\n",
    "trainer = models[mod]\n",
    "plt.plot(trainer.hist_val.acc_c_mcp, label = 'mcp - val')\n",
    "plt.plot(trainer.hist_val.acc_c_g, label = 'g - val')\n",
    "plt.plot(trainer.hist_val.acc_c_entropy, label = 'entropy - val')\n",
    "plt.plot(trainer.hist_val.acc_list, label = 'acc_0 - val')\n",
    "#plt.axhline(trainer.hist_val.acc_list[-1],color = 'tab:red', label = 'acc_0 - val')\n",
    "#plt.xlim(0,250)\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.plot(trainer.hist_train.acc_c_mcp, label = 'mcp - train')\n",
    "plt.plot(trainer.hist_train.acc_c_g, label = 'g - train')\n",
    "plt.plot(trainer.hist_train.acc_c_entropy, label = 'entropy - train')\n",
    "plt.axhline(trainer.hist_train.acc_list[-1],color = 'r', label = 'acc_0 - train')\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.plot(trainer.hist_val.g_list,label = 'g - val')\n",
    "plt.plot(trainer.hist_train.g_list,label = 'g - train')\n",
    "plt.title('variação da média de g')\n",
    "\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "plt.plot(trainer.hist_val.loss_list,label = 'Validation')\n",
    "plt.plot(trainer.hist_train.loss_list,label = 'Training')\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod = model_selective_def\n",
    "trainer = models[mod]\n",
    "plt.plot(trainer.hist_val.acc_c_mcp,'--', label = 'mcp - val')\n",
    "plt.plot(trainer.hist_val.acc_c_g,'--', label = 'g - val')\n",
    "plt.plot(trainer.hist_val.acc_c_entropy,'--', label = 'entropy - val')\n",
    "plt.plot(trainer.hist_val.acc_list,'--', label = 'acc_0 - val')\n",
    "#plt.axhline(trainer.hist_val.acc_list[-1],color = 'tab:red', label = 'acc_0 - val')\n",
    "mod = model_selective_def_no_head\n",
    "trainer = models[mod]\n",
    "plt.plot(trainer.hist_val.acc_c_mcp, label = 'mcp - val - noHead')\n",
    "plt.plot(trainer.hist_val.acc_c_g, label = 'g - val - NoHead')\n",
    "plt.plot(trainer.hist_val.acc_c_entropy, label = 'entropy - val - NoHead')\n",
    "plt.plot(trainer.hist_val.acc_list, label = 'acc_0 - val - NoHead')\n",
    "\n",
    "plt.xlim(0,200)\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "mod = model_selective_def\n",
    "trainer = models[mod]\n",
    "plt.plot(trainer.hist_train.acc_c_mcp,'--', label = 'mcp - train')\n",
    "plt.plot(trainer.hist_train.acc_c_g,'--', label = 'g - train')\n",
    "plt.plot(trainer.hist_train.acc_c_entropy,'--', label = 'entropy - train')\n",
    "plt.plot(trainer.hist_train.acc_list,'--', label = 'acc_0 - train')\n",
    "\n",
    "mod = model_selective_def_no_head\n",
    "trainer = models[mod]\n",
    "plt.plot(trainer.hist_train.acc_c_mcp, label = 'mcp - train - NoHead')\n",
    "plt.plot(trainer.hist_train.acc_c_g, label = 'g - train - NoHead')\n",
    "plt.plot(trainer.hist_train.acc_c_entropy, label = 'entropy - train - NoHead')\n",
    "plt.plot(trainer.hist_train.acc_list, label = 'acc_0 - train - NoHead')\n",
    "plt.xlim(0,200)\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = models[model]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod = model\n",
    "trainer = models[mod]\n",
    "plt.plot(trainer.hist_val.acc_list, label = 'acc_fg - val')\n",
    "plt.plot(trainer.hist_train.acc_list, label = 'acc_fg - train')\n",
    "#plt.axhline(trainer.hist_val.acc_list[-1],color = 'tab:red', label = 'acc_0 - val')\n",
    "\n",
    "mod = model\n",
    "trainer = models[mod]\n",
    "plt.plot(trainer.hist_train.acc_list, label = 'acc_f - train')\n",
    "plt.plot(trainer.hist_val.acc_list, label = 'acc_f - val')\n",
    "#plt.xlim(0,100)\n",
    "\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod = model\n",
    "trainer = models[mod]\n",
    "plt.plot(trainer.hist_train.acc_list, label = 'acc - train')\n",
    "plt.plot(trainer.hist_val.acc_list, label = 'acc - val')\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.plot(trainer.hist_train.loss_list,label = 'loss - Training')\n",
    "plt.plot(trainer.hist_val.loss_list,label = 'loss - Validation')\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod = model\n",
    "output,label = accumulate_results(mod,validation_dataloader)\n",
    "#w = normalize_tensor(g)\n",
    "#H = entropy_const(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "H.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod = model_selective\n",
    "mod.return_g = True\n",
    "output,label = accumulate_results(mod,test_dataloader)\n",
    "\n",
    "#acc = correct_total(output,label)/label.size(0)\n",
    "#g_list = []\n",
    "#mcp_list = []\n",
    "ideal = []\n",
    "for c in np.arange(0,1,0.05):\n",
    "    g_list.append(unc_comp.acc_coverage(output,label,1-g,c))\n",
    "    mcp = unc.MCP_unc(output)\n",
    "    mcp_list.append(unc_comp.acc_coverage(output,label,mcp,c))\n",
    "    ideal.append(min(1,acc/(1-c)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ideal = []\n",
    "mcp_list = []\n",
    "for c in np.arange(0,1,0.05):\n",
    "    mcp = unc.MCP_unc(output)\n",
    "    mcp_list.append(unc_comp.acc_coverage(output,label,mcp,c))\n",
    "    ideal.append(min(1,acc/(1-c)))\n",
    "\n",
    "\n",
    "plt.plot(np.arange(0,1,0.05),mcp_list,label = 'mcp')\n",
    "plt.plot(np.arange(0,1,0.05),ideal,label = 'ideal')\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mcp_list[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import fsolve\n",
    "def const_eq(lamb,g):\n",
    "    w = nn.functional.softmax(lamb*g)\n",
    "    m = torch.numel(g)\n",
    "    H = entropy(w)\n",
    "    const = H - torch.log(0.8*m)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
